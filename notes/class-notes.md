# MLSS 2017 Lecture Notes 

## Bernard Schölkopf - What is ML?

+ Leibniz - thought experiments about understanding laws for data
+ what does it mean to generalize
	- deduction problem
* statistical learning theory - demarcation problem

### Cybernetics

+ 1940s, Cybernetics Norbert Wiener, Cynernetics or control and communication in the animal and the machine
	* study of control and information processing rather than energy processing in machines and animals
+ macy conferences 1946-1953
+ [project cybersyn at allende government (chile, 1971 - 1973)](https://en.wikipedia.org/wiki/Project_Cybersyn)
+ mcculloch-pittts, formal neurons can emulate universal turing machines
+ hebbs - formal neurons
+ rosenblatt - the perceptron, a probabilistic model for information storage
	* first perceptron, 6 input pixels -> modifiable weights from error propagation
	* perhaps first example of learning weights
+ perceptron convergence theorem (1962)
+ limitations - xor problems
	* excessive learning times
+ minsky and papert (1969) - perceptrons
+ adam newey - CS as a principled discipline for inquiry
+ symbolic AI
	* process of manipulating discrete symbols
		- john mccarthy, allen newey, herb simon, marvin minsky
+ symbolic ai did lead to the birth of CS
	* led to development of high-level programming langauges (IPL and lisp)
+ the end of perceptrons
+ rosenblatt continued, but passed away in 1971
+ defeat of neural networks ligeitmized symbolic AI
+ neural network research continued at the fringe
	* kohonen, hinton, amari, grossberg
	* probabilistic reasonign in intelligent systems
+ in parallel, pattern recognition studying statistical learning theory development (international of control science at russia)
	* vapnik and chervonenkis (1968 - 1982)
+ expert systems / knowledge representations were made probabilistic
	* judea pearl (1988)
	* gave birth to bayesian networks - probabilistic graphical models
	* how to connect probabilities
+ backpropagation in 1980s
+ perceptrons 2nd edition
	* backprop simply form of calculating gradients
	* leads to solutions every time
	* just hill climbing
+ solomonoff (1950s) - probabilistic AI
+ vapnik - generalized portrait algorithm (mid 60s in his thesis)
	* some kind of optimal marginal for perceptron rule/algorithm
	* notion of positive definitive kernel (1904, hilbert)
+ CS is a discipline centered around programs
+ program can be written iff we have a pricise model of what it shoudl do 
+ human comes up with models (induction), computer does the rest (deduction)
+ nick bostrom - superintelligence
+ Vapnik paper, generalization of Glivenko-Cantelli
	* dudley: "shocking"


## Shai Ben-David - Understanding Machine Learning, A Theory Perspective

+ key ingredients
+ data distribution D
+ $f: x \to y$
+ minimize probability of $p_h(H(x) \ne f(x))$
+ natural measure: empirical error of h $\#S = |i: h(x_i) \ne f(x_i)|$
+ pigeon superstition (Skinner 1948)
	* aim to replicate human behavior in animals
	* pigeon experiment - replicate superstition
+ no free lunch
	* no learning is possible without prior knowledge
+ PAC Learnability - if there is a function $m_h : (0,1)^2 \to N$ and a learnign algoirthm A, such that for every distribution D over X, ever $\epsilon$, $\delta > 0$, and every f in H, for samples S of size $m>m_H(\epsilon, \delta)$ generated by D and labeled by f, 
	* $Pr[L_D((A(S)) > \epsilon ] < \delta$
+ independent of unknown distribution D
	* in statistics, often make assumptions on distribution D first
	* in ML, we are using arbitrary distribution D, bound still holds as it is
+ the rule depends on the classes
+ relaxing the realizability assumption
	* wish to model scenarios in which the learner does not have prior knowledge of a class to which the true classifer belongs
	* furthermore, often the labels are not determined by the instance attributes (not deterministic)
+ general loss: $\ell:H \times Z \to \mathbb{R}$
	* loss tells you how bad the model is given a point
	* $L_P (H) = \mathbb{E}_{X~P}(\ell(h,z))$
	* general loss tells you expected loss under given sample point
+ Agnostic PAC Learner
	* H is agnostic PAC lernable if there is a function $m_H:(0,1)^2 \to N$ and a learning algorithm A, such that for every distribution P over $X\times Y$ and every $\epsilon,\delta > 0$, for samples S of size $m > m_H(\epsilon, \delta)$ generated by P,
		- $Pr[L_P (A(S)) > Inf_[h \in H] L_P (h) + \epsilon] < \delta$
	* instead of making absolute statement that is guaranteed only under certain assumptions (like realizability), making a weaker, relative guarantee that is not much worse than the best in the class, and is guaranteed to always hold
+ **uniform convergence property** : 
+ If H is finite, then it has the uniform convergence property
+ any finite H, is agnostically PAC-learnable.
+ _proof_: hoeffding inequality implies uniform convergence property for single h's and then teh union bound handles the full class
+ can we not restrict to a class H, i.e., use a universal learner
	* no-free lunch theorem says no universal learner
	* Let A be any learnign algorithm over some domain set X
	* Let m be < |X| / 2 then there is a distribution P over $X \times {0, 1} and f: X \to {0, 1}$ such that
		1. $L_P(f)=0$
		2. For P-samples S of size m with probability > 1/7 $L_P(A(S)) > 1/8$

### Distinguishing between learnable and not learnable

+ some infinite classes are learnable
	* eg: 
		1. initial segments of the real line
		2. class of singletons over any domain set
+ a combinatorial characterization of PAC learnable classes
+ a class H shatters a domain subset A if for every susbet B of A there is some $h_B$ in H so that for all x in A $h_B(x)=1$ if and only fi x is in B. 
+ VC dimension:
	* largest set such that H shatters A
	* $VC_{dim_H} = \sup{|A|: H shatters A}$
+ The fundamental theorem: the following statemetns are equivalent
	1. H has the uniform convergence property
	2. ERM is an agonstic PAC learner for H
	3. H is agnostic PAC learnable 
	4. H is PAC learnable
	5. $VC_{dim_H}$ is finite

### Part III

#### Quantitative version of the fundamental theorem

+ $H$ has **uniform convergence property** with $C_1(d+\log(1/\delta))\epsilon^2 < m^{uc}_H(\epsilon,\delta)<C_2(d+\log(1/\delta))/\epsilon^2$
+ $H$ is **agnostic PAC learnable** with $C_1(d+\log(1/\delta))\epsilon^2 < m_H(\epsilon,\delta)<C_2(d+\log(1/\delta))/\epsilon^2$
+ $H$ is **PAC learnable** (_realizable case_) with $C_1(d+\log(1/\delta))\epsilon < m_H(\epsilon,\delta)<C_2(d+\log(1/\delta))/\epsilon$

**Example** Neural networks, VC dimension is about $|E|\times\log|E|$, where $E$ are number of edges/weights. Rearranging,
$C_1(d+\log(1/\delta))m^{uc}_H(\epsilon,\delta) < \epsilon^2 <C_2(d+\log(1/\delta))/m^{uc}_H(\epsilon,\delta)$,
or roughly $d/m$, where $d$ is sample-size and $m$ is VC-dimension, or edges. 

So if edges is order of magnitude the same or larger than training size, $\epsilon$ will be $\ge1$, no guarantees.

Hence, for guarantees need sample sizes with training examples that are order of magnitude larger than edges. This is worst-case theory. 

### Two missing components

+ classes are learnable only if they have finite VC dimensions: this might be too restricted
	- fixing the class with finite VC dimension is sometimes too limited
+ computational complexity
	- ERM's computational complexity in many cases is NP hard.

#### Relaxing the notion of learnability -- non-uniform learnability
+ A class $H$ is non-uniformly learnable if there is a function $m_H: H_x(0,1)^2 \to \mathbb{N}$ and a learning algorithm $A$, such that for every distribution $P$ over $X\times Y$ and every $\epsilon,\delta>0$, for every $h$ in $H$ for samples S of size $m>m_H(h,\epsilon,\delta)$ generated i.i.d. by $P$, 
$$
Pr[L_P(A(S))>L_p(H)+\epsilon]<\delta
$$
- no longer uniform in $m_H$, different number of necessary samples depending on the $h$.

+ If $H$ shatters an infinite set, then it is not even non-uniform learnable
	- in particular, the class of ALL functiosn over any finite domain is not non-uniform learnable.
	- it shatters $\mathbb{N}$

### Three Missing Topics

1. Safety - our ERM results relied on statistical guarantees. Some use-cases can not tolerate $\epsilon$ errors. Here we can'
2. Fairness - examining predictions based on past data.
3. Interpretability - understanding an interpreting model results.


## Bernard Schölkopf - Causality

+ Storks delivers our babies
+ Reichenbach - Common cause principle
	* book: the direction of time
	1. if X and Y are statistically dependent, then there exists $Z$ causally influencing both of them
	2. Z screens X and Y from each other, (given Z, X and Y become independent)

+ SCM - structural causal model
+ $A := N_A$
+ $T := f_T(A,N_T)$
	* where $N_T$ independent of $N_A$
+ allows identification of the causal graph under suitable restrictions on the functional form of $f_T$.
+ Structural causal model (Pearl et. al)
+ directed acyclical graph with vertices
+ semantics: vertices = observables, arrows = direct causations
+ $X_i := f_i(PA_i,U_i)$ with indepdent RV $U_1,\ldots,U_n$, where U stands for unexplained random variabels
	* also called a nonlinear structural equation model 

## D. Janzing - Causality

+ Causal structure formalized by DAG $G$ with random variables $X_1,\ldots,X_n$ as nodes
+ Causal markov condition states that the density $p(x_1,\ldots, x_n)$ then factorizes into 

$$
p(x_1,\ldots, x_n) = \prod^n_j=p(x_j| pa_j)
$$

+ Pearl's do-notation, distribution of $Y$ given that $X$ is set to $x$:
	- $p(Y|do \, X = x)$ or $p(Y|do \, x)$

+ Computing $p(X_1,\ldots,X_n|do \, x_i)$ from $p(X_1,\ldots,X_n)$ and $G$
	- start with causal factorization
	- replace conditionals for intervention variables by Kronecker delta
		* i.e., replace $p(X_i|PA_i)$ with $\delta_{X_i,x_i}$

### Inferring the DAG

+ Key postulate: causal markov condition
+ Essential concept: d-separation
+ Describing conditional independencies using paths and blocks along paths
	- d-separation provides the descriptive notion of conditional independence
+ Berkson's paradox (1946): independence variables, but correlated through confounding
+ (Reichenbach 1956): asymmetry under inverting arrows

## Ben Schölkopf - Causality Part II

## Max Welling - Marrying Graphical Models & Deep Learning

+ Main actor of today's story:

$$
\mathbb{E}_{Q(V)}[\log P(X|V)] - KL[Q(V)||P(V)]
$$

+ $P(V)$ is complexity penalty

### ML as Computational Statistics

+ There are perspectives from statistics that you cannot get from an optimization perspective
+ Maximize log-likelihood

$$
\max_\Theta \log P(X_1,\ldots,X_n|\Theta)
$$

for unsupervised.

For supervised:

$$
\max_\Theta \log P(Y_1,\ldots,Y_n|X_1,\ldots,X_n|\Theta)
$$

and minimization of loss:

$$
\min_\Theta \sum_i Loss(Y_i,\hat{Y}(X_i,\Theta))
$$

### Bias-Variance Decomposition

+ Examining $Y=f(x)+\epsilon$, where $\epsilon \sim \mathcal{N}(0,\sigma_\epsilon)$
+ $Err(x)=\mathbb{E}[(Y-\hat f(x))^2]$
+ $Err(x)=(\mathbb{E}[\hat{f}(x)]-f(x))^2 + \mathbb{E}[(\hat(f)(x)-\mathbb{E}[\hat(f)(x)])^2] + \sigma_\epsilon ^2$
+ First term is variance, second is bias, third is irreducible error

## Graphical Models

+ Concisely represent conditional independence relations between variables
+ One-to-one correspondence between the dependencies implied by the graph and the probabilistic model
+ Can do calculations just by marginalizing out the graph


### Bayes Ball Algorithm

+ mechanically/mechnanistically if variables are marginally independent
+ An undirected path is active if a Bayes ball traveligna long it never encounters the "stop" symbol
+ If there are no active paths from $X$ to $Y$ when ${Z_1,\ldots,Z_k}$ are shared, then $X\perp Y$.

### Markov Random Fields

+ Probability distribution as maximal clique: 

$$
P(X) = \frac{\prod_c\Phi_c(X_c)}{Z}
$$

### Latent Variable Models

+ Introduction latent (unobserved) variables (perhaps confounders, if taking a causal perspective) will dramatically increase the capacity of the model:
$$
P(X) = \sum_Z P(X|Z)P(Z)
$$
+ Fundamental degrees of freedom of what you're trying to model
+ Problem: $P(Z|X)$ is intractable for most nontrivial models
	- for learning/inference, you need $P(Z|X)$ (unobserved nodes given observed nodes), so this can be tricky

### Mainstream Ways of Handling Intractable Inference

#### Variational Inference

+ Want to estimate some complex probability distribution $p$
+ Restrict yourself to a family of simple distributions $Q$
+ **Advantages**:
	- deterministic
	- easy to assess convergence
+ **Disadvantages**:
	- biased
		* Never get actual $P$, since you're biased
	- Local minima

#### Sampling - MCMC

+ **Advantages**:
	- Unbiased

+ **Disadvantages**:
	+ Stochastic (sample error)
		- suffering from variance, not bias this time
	+ hard to mix between modes
	+ Hard to assess convergence

#### Independence Samplers and MCMC

+ Genearting independent samples: sample from $g$ and suppress samples with low $p(\theta | X)$, e.g., rejection sampling, or importance sampling
	- does not scale to high dimensions
	- too much variance
+ MCMC
	- make steps by perturbing previous sample
	- probability of visiting a state is equal to $P(\theta | X)$
+ Sampling 101: Metropolis-Hastings
	- propose new step with Gaussian movements
	- satisfies detailed balance: is probability flow in either transition balanced
	- is it easy to come back to the current state?
	- is the new state more probable
	- Burn-in is unnecessarily slow
	+ This algorithm is $\mathcal{O}(N)$

#### Variational Inference

+ Choose tractable family of distributions
+ Minimize $Q: KL[Q(Z|X)||P(Z||X)]$
+ Equivalent to maximize of $\Phi$:

$$
\sum_Z Q(Z|X,\Phi)(\log P(X|Z,\Theta)|P(Z) - \log(Q(Z|X,\Phi)))
$$

+ in learning, maximize the probability of observed data given parameters
+ KL provides notion of bound $B: KL[Q(Z|X)||P(Z||X)]$
+ E-M:
	1. E-Step: $\arg \max_\Phi B(\Theta,\Phi)$ [variational infernece]
	2. M-step: $\arg \max_\Theta B(\Theta, |\Phi)$ [approximate learning]

+ when no gap, then EM, otherwise variational inference
+ coordinate ascend on bound

#### Amortized Inference

+ Encoder: $q_\phi(z|x|)$
+ decoder: $z\sim p_\theta (z)$
+ parameters $\phi$ are shared across all data points

### Relations between graphical models and deep learning

+ Start with some interest in an object $P(Y|X)$
	- could be as complicated as we want
	- say a deep neural network
		* just a glorified conditional distribution in a graphical model

#### Deepify Operator

+ Sam Roweis: "Much better to invent an operator, than a new model. Model: 1 paper, Operator: long string of papers"
+ "Deepify operator" - pick a graphical model with conditional distributions and replace those with a deep neural network
+ Logits: deep NN
+ Deep survival analysis: replace Cox's proportional hazard function with a deep network

#### Deep Genrative Model: The Variational Auto-Encoder

+ Hemholtz machine (80s)
+ read old Geoffrey Hinton's papers and reinvent them
+ we can now reintroduce his ideas 
+ deterministic NN node -> unobserved stochastic node -> observed stochastic node


#### Wake-Sleep Algorithm

+ Stochastic variational Bayesian inference


#### Reducing the variance: reparameteriztion trick

+ want to reduce variance of gradients
+ Taking noise, and write variable as a deterministic function of parents and some standard normal error


### Discriminative vs Generative

#### Generative

1. Inject expert knowledge
	- impose prior constraints
	- great for low data regimes
2. Interpretable
3. Domain transfer robust


#### Discriminative

1. Solves the problem you care about
	- solves the map you care about
	- rather than a different map that you must invert
2. Highly successful accurate
3. Less need to impose prior constraints, needs lots of data for estimating

### Deep Convolutional Networks

+ really good for signal processing
	- looks at low-signal data
+ CNNs:
	- local filters
	- weight sharing
	- more global, abstract and invariant representations in deeper layers
+ three applications of transfer learning for medical examples to super-human-performance
	- dermatology (thrun et. al)
	- breast cancer detection
	- retina lesions
+ disadvantages:
	1. way too big
	2. consume too much energy
	3. use too much memory

### Bayesian Deep Learning

+ Automatic model selection / pruning
+ Automatic regularization
+ Realistic prediction uncertainty (important for decision making)

### Variational Bayes

+ One integral for every dataset


### Sparsifying and Compressing CNNs

+ Owed to Geoffrey Hinton's old papers
+ DNNs are vastly overparameterized (e.g., distillation, Bucilua et. al 2006)
+ simple idea, learn a soft weight sharing prior (Nowlan & Hinton 1991, Gong et al 2014)
+ intepret variational bound as coding cost for data (minimum description length)
+ use a mixture gaussian prior
	- train weights

#### Empirical Bayes

+ Fit mixture of gaussians prior to the distribution of weights

$$
p(w) = \prod^I _{i=1} \sum^J _{j=0} \pi_j \mathcal{N}(w_i | \mu_j, \sigma_j ^2) 
$$

- fixed component at $w=0$ encourage to be very large (large $pi_0$)


## Ruslan Salakhutdinov - Deep Learning

### Supervised (Discriminative) Learning

+ ERM: 

$$
\arg \min_\theta\frac1T\sum_T \ell (f(x^{(t)}; \theta)y^{(t)} + \lambda \Omega(\theta))
$$

### Stochastic Gradient Descent

+ Perform updates after seeing each example:
	- initialize $\theta \equiv {W^{(1)}, b^{(1)}, \ldots, W^{(L+1)}, b^{(L+1)}}$
	- 
+ Momentum: exponential average of previous gradients.


### Learning Distributed Representations

+ deep learning: learning modules with multilayer representations
	- multilayer (feed-forwad) neural networks
	- multilayer graphical model (deep belief net, deep Boltzmann machine)
+ each layer learns distrbuted representation
	- units in a layer are nto mutualaly exclusive
		* each unit is a separate feature of a the input
		* two units can be active at the same time
	- units do not correspond to 

### Best Practice

+ Given a dataset $\mathcal{D}$, pick a model that overfits it entirely
	- gets 0 training error
+ Regularize the model, e.g., using dropout
+ SGD with momentum, batch-normalization, and dropout usually works very well
	- hard to beat

### Dropout

+ Cripple neural network by removing hidden units **stochastically**
	- each hidden unit is set to 0 with probably 1/2
	- prevents hidden units from co-adapting
	- hidden units must be more generally useful
		* at any point in time, any given can be shutdown
		* forces the model to learn redundancies, or features that can discriminate particular classes especially well

+ At test time, replace the masks by their expectations
	- this is simply the constant vector 0.5 if dropout probability is 0.5
	- For single hidden layer, equivalent to taking the geometric average of all nueral networks, with all possible binary masks

### Residual Networks

+ really hard to train deep networks
	- worse error on both training and test sets compared to shallower nets!
- key idea: use skip-connections, introduce pass through into each layer
	- thus, only residual needs to be learned


## Zoubin Ghahramani - Probabilistic Machine Learning: Foundations and Frontiers

### Neural Networks as a Probabilistic Model

+ Neural networks are tunable nonlinear functions with many parameters
+ Parameters $\theta$ are weights of a neural network
+ Neural networks model $p(y^{(n)} | x^{(n)},\theta)$ as a nonlinear function of $\theta$ and $x$.
+ Multilayer neural networks model the moverall function as  composition of functions (layers)
+ Usually trained to miximize likleihood (or penalized likelihood) using variants of stochastic gradient descent optimization

**what's a neural network**: nonlinear functions + basic stats + basic optimization

### Revolution in Neural Networks

1. Algorithmic and architectural innovations - (many layers, ReLUs, dropout, LSTMs)
2. vastly larger data sets (web-scale)
3. vastly larger-scale compute resources (GPU, cloud)
4. much better software tools (autodifferentiation)
5. vastly increased industry investment and media hype

### Issues

+ poor at representing uncertainy
+ easily fooled by adversarial examples
+ finicky to optimize: non-convex + choice of architecture, learning procedure, initialization, etc., require expert knowledge and experimentation
+ uninterpretable black-boxes, lacking in transperancy, making them difficult to trust

### Beyond Deep Learning - Toolboxes versus Modeling

+ Machine learning as a tooolbox of methods for processing data: feed data into one of many methods, choose methods that have good theoretical or empirical performance, make predictions and decision
+ Machine learning as a science of learning models from data: define a space of possible models, learn the parameters and structure of the models from data, make predictions and decision
+ A model describes data that one could observe from a system
+ If we use the mathematics of probability theory to express all forms of uncertainy and noise associated with our model...
+ ... then inverse probability (i.e., Bayes rule) allows us to infer unknown quantities, adapt our models, make predictions and learn from data.

### Bayesian Machine Learning

**Learning**: 
$$
P(\theta|\mathcal{D},m)=\frac{P(\mathcal{D}|\theta,m)P(\theta|m)}{P(\mathcal{D}|m)}
$$

**Prediction**:

$$p(x|D,m) = \int P(x|\theta,D,m)P(\theta,d,m)$$

**Model comparison**:

$$
P(m|D) = \frac{P(D|m)P(m)}{P(D)}
$$

Why care about Bayesian learning

+ we want calibrated model and prediction uncertainty: getting systems that know when they don't know
+ Automatic model complexity control and structure learning (Bayesian Occam's Razor)

### Algorithms vs Models

+ Some algorithms target finding a parameter optimum, $\theta^\star$
+ other algorithms target inferring the posterior $p(\theta|D)$
+ often these are not so different
+ Bayesian methods are **algorithms**

### Bayesian Deep Learning

+ Laplace approximations (MacKay, 1992)
+ Variational approximations (Hinton and van Camp, 1993, Graves 2011)
+ MCMC (Neal, 1993)
+ SGLD (Welling and Teh, 2011)
+ Probabilistic back-rop (Hernandez-Lobato, et. al 2015, 2016)
+ Dropout as model averaging (Gal and Ghahramani, 2015, 2016)

#### When Uncertainty Helps

+ Extrapolation time - being able to average over many models can be exceptionally fruitful

### When is the probabilistic approach essential?

+ Learnign and intelligence depend crucially on the caterful probabilistic representation of uncertainty
	1. forecasting
	2. decision making
		- evaluating decisions in a stochastic world
	3. learning from limited, noisy, and missing data
		- non-imagenet examples
		- medical examples with limited datasets
		- not-okay to get things right on-average
	4. learning complex personalized models
	5. data compression
		- every lossless compression algorithm, is implicitly/explicitly representing a probabilistic model/approximation
	6. automatic modeling, discovery, and experiment design

### Automating Machine Learning

+ probabilistic programmign - automating inference
+ automatic statistician - automating model discovery and evaluation
+ automatic optimization - Bayesian optimization, optimizing hyperparameters

#### Probabilistic Programming 

+ **Problem** - probabilistic model development and derivation of inference algorithms is time-consuming and error-prone
+ **Solution**:
	1. develop **probabilistic programming languages** 
		- expressing probabilistic models as computer programs that generate data (_simulators_)
		- should be able to express and computable probability distribution in a Turing complete system
	2. derive **universal inference engines for these languages** 
		- do inference over program traces given observed data (_Bayes rule on computer programs_)
		- think of computer vision as inverse graphics
		- speech: analysis by synthesis
			* understanding the mapping from vocal tract - soundwave, analyze by going backwards
+ probabilisitc programming languages implementations:
	- church, anglican, turing: completely expressive language, Turing complete
		* inference could be computationally intractable
	- infer.net, BUGS, stan - slightly more limited, but much more efficient inference
	
## Jure Leskovec - Network Analysis

### Problem 1 - Computing with Network Data

+ Networks provide a common language for a variety of problems
	- social networks
	- proteins
	- film - actor connections
+ how do you define a network
	- what are the ndoes
	- what are the edges
+ choice of the proper network representation of a given domain/problem determines ouro ability to use networks succesfully
	- in some cases, there is a unique, unambiguous representation
	- some times, it might not be unique
+ SNAP
	- general purpose, high-performance system
	- C++, python (BSD, open source)
	- snap.stanford.edu
+ ability of hardware
	- networks fit in a RAM of a single machine
	- a billion edges
	- using 1TB machines
+ SNAP graph analytics workflow:
	- Raw data -> structured data -> networks
	- in mmeory graph containers
	- graph table conversions
	- secondary storage
+ Resources:
	- [github source](https://github.com/snap-stanford/snap)

### Machine Learning in Networks

+ Goal: efficient task-independent feature learning for machine learnign in networks:
	- for any node: $f:u\to\mathbb{R}^d$ mapping to a feature emebdding
	- can we do representation learning for neworks?
+ Why is it hard?
	- graph representations are hard
	- images are fixed size
		* GCN
	- text is linear
		* word2vec
	- Graphs are neither
		- node numbering is arbitrary
			* node isomorphism problem
		- much more complicated structure
+ Ideas
	- linearizing the graph
		* create a "sentence" for each node using random walks
			- node2vec, OhmNet
	- graph convolutional networks
		+ propagate information between the nodes of the graph
			- graphsage

### Node2Vec - Random Walk Based (Unsupervised Feature Learning)

+ **Intuition** - find embedding of nodes to $d$-dimension space that preserves similarity
+ **Idea** - Learn embedding such that _nearby_ nodes are close together
+ Given a node $u$, how do we define nearby nodes?
	- $N_S(u)$ ... neighborhood of $u$ obtained by some strategy $S$.

#### Unsupservised Feature Learning

+ Goal: find emebdding f(u) that predicts neary nodes N_S(U)
$$
\max_f \sum_{u\in V}\log Pr(N_S(u)|f(u))
$$

+ make indepndence assumption
$$
Pr(N_S(U)|f(U)) = \prod_{n_i \in N_S(U)} Pr(n_i|f(u))
$$

+ Then softmax

$$
Pr(n_i|f(u)) = \frac{\exp(f(n_i) \cdot f(u))}{\sum_{v\in V}\exp(f(v)\cdot f(u))}
$$


+ estimate $f(u)$ using SGD

#### How to determine $N_S(U)$

+ Two classic strategies to fine a neighborhood of a given node $u$, use BFS, or DFS
+ BDFS: local microscopic view
+ DFS: Global macroscopic view

#### Interpolating between BFS and DFS

+ Biased random walk $S$ that given a node $u$ generates a nhood $N_S(u)$
+ Biased 2nd-order random walk explore network neighborhoods
	- BFS-like: low value of $p$
	- DFS-like: low value of $q$

#### `node2Vec` algorithm

1. Compute random walk probabilities
2. Simulate $r$ random walks of length $\ell$ starting from each node $u$
3. Optimize the `node2vec` objective using SGD

Advantages:
	1. linear-time complexity
	2. All 3 steps are individually parallel

+ Other walk-based methods:
	1. LINE
	2. DeepWalk
	3. Structural Deep Network Embedding

### Hierarchical Networks

#### Multilayer Networks

+ Each network is a layer $G_i=(V_i,E_i)$.
+ Simliarities between layers are given in hierarchy $\mathcal{M}$, map $\pi$ encodes parent-child relationships
+ **The Approach**: computational framework 
+ Features in a multi-layer network
	- given layers ${G_i}$, hierarchy $\mathcal{M}$
		layers are in leaves of $M$
	+ Goal: learn functions $f_i: V_i \to \mathbb{R^d}$
+ Per-layer objectiev: node2vec
	- intutition, for each layer find a feature represntation for predicting nearby nodes

### GraphSage: Supervised Feature Learning

+ Want to scale to large networks
	- generaliaze to new nodes
+ Inductive feature learning on networks
+ Inductive node embedding -> generalize to entirely unseen graphs
+ What makes this hard?	
	- Generalizing to unseen subgraphs requires "aligning" the new subgraph to data we've seen before	
		* closely related to subgraph isomorphism problem
+ Graph convolutional netwo	rks
	- generalize convolutions beyone simple lattices
	- leverage node features/attributes (e.g., text, degrees)
+ ICML 2016: Ahmed & Kutzkov - learning convolutional neural neworks for graphs
	- take a node, find a neighborhood, normalize to find canonical nhood, receptive field operation to obtain input for CNN
+ GCN's + Aggregation: Kipf et. al (2017) 
	- semi-supervised learnign on graphs

$$
f(X,A) = \text{softmax}(\hat A \text{ReLU} (\hat A XW^{(0)})W^{(1)})=\max (0,x)
$$

## Zoubin - Day Two

### Bayesian Optimization

+ optimizing a black-box function is expensive to evaluate

$$
x^\star = \arg \max_x f(x)
$$

+ treat the problem as a sequential decision-making and model uncertainy in the function
	- uncertainty - don't know the values of the function at parameter values you haven't seen yet

+ Black-box optimization in a nutshell:
	1. initial sample
	2. initalize our model
	3. get the acquisition function $\alpha(x)$
		- criterion
		- needs a rep of uncertainty
		- could be thought of as multi-armed bandits
	4. optimize it!: $x_{next} = \arg \max \alpha(x)$
	5. sample new data, update model
	6. repeat!
	7. make recommendation

+ examining current function values, what's the promising places to evaluate next
	- what's the probability of getting an improvement in a new location?
	- another criterion: expected improvement
	- alternatively: information gain
+ the correct way of solving thsi as any sequential decision problem
	- if we know the horizon, work backwards from the end and solve the multistep decision problem (not using greedy decisions)
	- computationally intractable
+ A good framework for thinking about any optimization problem. Especially useful :	
	1. evaluating function is expensive
	2. evaluating derivatives is hard/impossible
	3. there is noise in the function evaluations
	4. possible noisy constraints
	5. prior information about the function	
		- valentin del avart (PhD thesis)- probabilistic programs as models of the function to optimize to optomize computer systems
	6. one needs to optimize many similar functions
 
### Automatic Statistician - Automating model discovery

+ Can we automate model discovery from data?
	- more ambitiously - autoamte the entire pipelien: data processing, model search, evaluation, and explanation
+ need a language of models that are expressive enough about the description of 	models
	- general enough to explore rich models
	- expressive enough to capture real-world phenomena


## Gassian processes

+ For any subset of points the marginal distribution over that subset is multivariate Gassian 
+ Can be thought of Bayesian linear regression in the Kernel feature space
+ Radford Neal one layer with infinite units lead to GPs
Neural network kernel function 

## Jure Leskovec - Network Analysis Part II

### Community Detection

+ Given a network, can we detect community membership of nodes
+ Goal: find sets of nodes that are densely connected with each other
+ Define an objective function $\phi(s)% that creates "good" clusters $S$.
- Conductance:

$$
\phi(S) = \frac{\#\text{edges cut}}{\text{vol}(S)}
$$

wheres $\text{vol}(S)=\text{(number of edge end points in S)}$

+ Existimg methods assume that edge probability _decreases_ with the umber of shared communities

#### Comunity-Affiliation Graph

+ Generative Model: How is a network generated from community affiliation
	- later we detect comunities by fitting them to the generative model
	- Union of Erdos-Renyi graphs
+ Model parameters $B(V,C,M,{p_c})$:
	- nodes $V$, communities $C$, memberships $M$
+ Likelihood of graph given network:

$$
\arg \max_B P(G;B) = \prod_{((i,j)\in E)} P(i,j) \prod_{(i,j)\ni E} (1-P(i.j))
$$

where $P(i,j) = 1 - \prod_{c\in M_i\cap M_j}(1-p_c)$

#### Relaxing AGM

+ Relax the AGM: memberships have strengths
+ Let $F_{uA}$: the membership strength of node $u$ to community $A$.

#### BigCLAM Model

+ Probability of nodes linking is proportional the strengths of shared memberships:

$$
P(u,v) = 1- \exp(-F_u \cdot F_v^\top)
$$

+ highly nonconvex
	- could use gradient based optimization techniques for local optimation
+ Now given a network, we estimate F using log-likelihood:

$$
\ell(F) = \sum_{(u,v)\in E}  \log( 1- \exp(-F_u \cdot F_v^\top)) - \sum_{(u,v)\ni E}F_uF_v^\top
$$

+ Non-negative matrix factorization
	- Update $F_{uC}$ for node $u$ while fixing the memberships of all other nodes
	- Updating takes linear time in the degrees of $u$
+ 












&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;



&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;



&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

&nbsp;

&nbsp;


&nbsp;
&nbsp;
&nbsp;
&nbsp;